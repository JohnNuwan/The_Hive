# Tech Spec: AI & LLM Evaluation (The Judge)

## üß† 1. Concept
L'intelligence d'EVA doit √™tre mesurable pour justifier les mises √† jour de mod√®les. L'√©valuation porte sur la pertinence, la pr√©cision et l'absence d'hallucination.

## üèóÔ∏è 2. Framework d'√âvaluation

### A. Benchmarks Cognitifs
*   √Ä chaque changement de LLM (ex: Llama-3 -> DeepSeek-V3), EVA doit repasser une mini-batterie de tests :
    *   **Logic** : R√©solution de 5 probl√®mes de logique complexes.
    *   **Coding** : √âcriture d'un script Rust fonctionnel respectant les normes du projet.
    *   **Compliance** : 10 sc√©narios o√π l'on teste si elle respecte la Constitution.

### B. D√©tection d'Hallucination (RAG Check)
*   Pour les Experts utilisant la base de connaissance (*The Researcher*, *The Sage*), on utilise un **"Critic Model"** :
    1.  Mod√®le A g√©n√®re une r√©ponse bas√©e sur un document.
    2.  Mod√®le B (Le Juge) v√©rifie si chaque affirmation de la r√©ponse est pr√©sente dans le document source.
    3.  Si Score < 90% -> **Rejet**.

### C. Latence & Performance Token
*   Mesure du "Time to First Token" (TTFT).
*   Seuil Genesis : < 500ms pour une interaction Nexus.

## üõ°Ô∏è 3. Humain-dans-la-boucle (Human-in-the-loop)
*   Les r√©ponses d'EVA ont un bouton "Pouce lev√©/baiss√©" dans le Nexus.
*   Toute r√©ponse avec un "Pouce baiss√©" est automatiquement envoy√©e dans le dataset de *Fine-tuning* pour la prochaine it√©ration d'entra√Ænement.

## üóìÔ∏è Roadmap
*   **Phase 1** : Int√©gration de `Promptfoo` pour tester les prompts.
*   **Phase 2** : Pipeline automatis√© de scoring RAG.
